{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cc9ef3",
   "metadata": {},
   "source": [
    "# Demo Notebook for transformers models\n",
    "*SSC, May 2023*\n",
    "\n",
    "This notebook demonstrates the preliminary use for training transformers models. For now, all the methods are called from the notebook. In the future, a more user-friendly user interface will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ignore this cell: extra install steps that are only executed when running the notebook on Google Colab\n",
    "# flake8-noqa-cell\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.isdir('Test_Data'):\n",
    "    # we're running on colab and we haven't already downloaded the test data\n",
    "    # first install pinned version of setuptools (latest version doesn't seem to work with this package on colab)\n",
    "    %pip install setuptools==61 -qqq\n",
    "    # install the moralization package\n",
    "    %pip install git+https://github.com/ssciwr/moralization.git -qqq\n",
    "\n",
    "    # download test data sets\n",
    "    !wget https://github.com/ssciwr/moralization/archive/refs/heads/test_data.zip -q\n",
    "    !mkdir -p data && unzip -qq test_data.zip && mv -f moralization-test_data/*_Data ./data/. && rm -rf moralization-test_data test_data.zip\n",
    "    !spacy download de_core_news_sm\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7beae1",
   "metadata": {},
   "source": [
    "Import the required classes from the moralization package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moralization import DataManager, TransformersModelManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d63a6",
   "metadata": {},
   "source": [
    "### Import training data using DataManager\n",
    "\n",
    "If you need more information about raised warnings run: <br>\n",
    "```import logging ``` <br>\n",
    "```logging.getLogger().setLevel(logging.DEBUG)```\n",
    "\n",
    "Note that currently only annotations of one file, the one specified in `example_name` (see below) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on small dataset\n",
    "data_manager = DataManager(\"../data/Test_Data/XMI_11\")\n",
    "# train on full dataset\n",
    "# data_manager = DataManager(\"/content/data/All_Data/XMI_11\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, doc in data_manager.doc_dict.items():\n",
    "    print(f\"  - {title}: {len(doc)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40269a",
   "metadata": {},
   "source": [
    "## Prepare the data in dataset format\n",
    "The data is read in as xmi and then converted to a spacy doc object. This is done so we can specify the spans in the flowing text; and also that sentence boundaries are detected. For the transformers models, we feed the data in chunks, and currently each sentence is a chunk. One could also think about different choices such as paragraphs or instances.\n",
    "\n",
    "The doc object is generated by the `DataManager`. We then need to use the transformers specific methods in the `TransformersDataHandler` to create nested lists of tokens (nesting by sentences, these are the \"chunks\"), and make sure that the labels for the selected annotation are nested in the same way. The labels that are then assigned are \"2\" for the first token in an annotation, \"1\" for a token inside an annotation, \"0\" for no annotation, \"-100\" for punctuation marks as these should be ignored in the calculation of the loss function (cross entropy).\n",
    "This is all taken care of by the `DataManager`.\n",
    "\n",
    "1. xmi data -> spacy doc object\n",
    "2. get tokens, sentences and labels from spacy doc object and put in nested lists\n",
    "\n",
    "Now we convert the nested lists into a pandas dataframe. This dataframe can then be exported into a Hugging Face dataset and can be pushed to the hub.\n",
    "\n",
    "3. Nested lists into dataframe\n",
    "4. Dataframe to dataset\n",
    "5. Optional: Push dataset to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data in the DataManager\n",
    "data_manager.docdict_to_lists()\n",
    "data_manager.lists_to_df()\n",
    "# split the data into test and training set\n",
    "data_manager.df_to_dataset(split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e90ee7",
   "metadata": {},
   "source": [
    "You can now publish the dataset to the Hugging Face Hub. For this you either need to set the environment variable `HUGGING_FACE_TOKEN` or you can provide it here using the `hugging_face_token` keyword. The `repo_id` variable specifices the name of the repository that you want to use (or create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now push to hub\n",
    "data_manager.push_dataset_to_hub(repo_id=\"test-data-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe29b75",
   "metadata": {},
   "source": [
    "You can also update the metadata in the `DatasetInfo` object that goes along with your dataset. Possible options to update are `description`, `version`, `license`, `citation`, `homepage`. You can update one or several of these, or all of them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = data_manager.set_dataset_info(version=\"0.0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335de859",
   "metadata": {},
   "source": [
    "To update the dataset on Hugging Face Hub, you may now push this updated dataset, directly providing the updated dataset as a keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.push_dataset_to_hub(repo_id=\"test-data-3\", data_set=updated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a0319",
   "metadata": {},
   "source": [
    "## Get started with training a transformers model\n",
    "For this you need a model that you want to base your training on. You also need to provide the path to the directory where you want to save the model. The model name can be given using the `model_name` keyword. This keyword defaults to `bert-base-cased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm = TransformersModelManager(model_path=\".\", model_name=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4baecc",
   "metadata": {},
   "source": [
    "To train, simply call the `train` method with the above `data_manager`. The token and column names are passed using the `token_column_name` and  and `label_column_name` keywords. If the data has been prepared by the `DataManager` and was not a dataset you pulled from the Hugging Face Hub, these are set to `Sentences` and `Labels`. The number of training epochs is set by the keyword `num_train_epochs`.\n",
    "As optimizer we currently use AdamW. The learning rate can be adjusted directly using the `learning_rate` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79668aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_column_name = \"Sentences\"\n",
    "label_column_name = \"Labels\"\n",
    "num_train_epochs = 1\n",
    "learning_rate = 1e-5\n",
    "tmm.train(data_manager, token_column_name, label_column_name, num_train_epochs, learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbc7ac",
   "metadata": {},
   "source": [
    "You can now evaluate the model with an example phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = tmm.evaluate(token=\"Jupyter Notebooks sind super.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe052e",
   "metadata": {},
   "source": [
    "Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in evaluation_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26404884",
   "metadata": {},
   "source": [
    "The model is now saved in your provided `model_path`. We will add a functionality to push the model to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97f089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
