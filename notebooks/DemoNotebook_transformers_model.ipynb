{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cc9ef3",
   "metadata": {},
   "source": [
    "# Demo Notebook for transformers models\n",
    "*SSC, May 2023*\n",
    "\n",
    "This notebook demonstrates the preliminary use for training transformers models. For now, all the methods are called from the notebook. In the future, a more user-friendly user interface will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ignore this cell: extra install steps that are only executed when running the notebook on Google Colab\n",
    "# flake8-noqa-cell\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.isdir('Test_Data'):\n",
    "    # we're running on colab and we haven't already downloaded the test data\n",
    "    # first install pinned version of setuptools (latest version doesn't seem to work with this package on colab)\n",
    "    %pip install setuptools==61 -qqq\n",
    "    # install the moralization package\n",
    "    %pip install git+https://github.com/ssciwr/moralization.git -qqq\n",
    "\n",
    "    # download test data sets\n",
    "    !wget https://github.com/ssciwr/moralization/archive/refs/heads/test_data.zip -q\n",
    "    !mkdir -p data && unzip -qq test_data.zip && mv -f moralization-test_data/*_Data ./data/. && rm -rf moralization-test_data test_data.zip\n",
    "    !spacy download de_core_news_sm\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7beae1",
   "metadata": {},
   "source": [
    "Import the required classes from the moralization package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moralization import DataManager, TransformersModelManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d63a6",
   "metadata": {},
   "source": [
    "### Import training data using DataManager\n",
    "\n",
    "If you need more information about raised warnings run: <br>\n",
    "```import logging ``` <br>\n",
    "```logging.getLogger().setLevel(logging.DEBUG)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on small dataset\n",
    "data_manager = DataManager(\"/content/data/Test_Data/XMI_11\")\n",
    "# train on full dataset\n",
    "# data_manager = DataManager(\"/content/data/All_Data/XMI_11\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, doc in data_manager.doc_dict.items():\n",
    "    print(f\"  - {title}: {len(doc)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88b4aa",
   "metadata": {},
   "source": [
    "The default task that is trained on is task 1: Detection of moralization constructs (category I). If you want to train for a different task or label, you can specify it as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb012964",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"task2\"\n",
    "selected_labels = \"all\" # select all the labels for the given task\n",
    "data_manager = DataManager(\"/content/data/Test_Data/XMI_11\", task=task, selected_labels=selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723ce3f",
   "metadata": {},
   "source": [
    "The tasks are defined as:\n",
    "```\n",
    "\"task1\": [\"KAT1-Moralisierendes Segment\"]\n",
    "\"task2\": [\"KAT2-Moralwerte\", \"KAT2-Subjektive AusdrÃ¼cke\"]\n",
    "\"task3\": [\"KAT3-Rolle\", \"KAT3-Gruppe\", \"KAT3-own/other\"]\n",
    "\"task4\": [\"KAT4-Kommunikative Funktion\"]\n",
    "\"task5\": [\"KAT5-Forderung explizit\"]\n",
    "```\n",
    "You can select one of the tasks and all the labels for that task by setting `selected_labels=\"all\"`, or you can specify selected labels for a given task, for example if you selected `task=\"task2\"`, the labels can be given as a list `selected_labels=[\"Fairness\", \"Cheating\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ee7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"task2\"\n",
    "selected_labels = [\"Fairness\", \"Cheating\"] # select only the specified labels for the given task\n",
    "data_manager = DataManager(\"/content/data/Test_Data/XMI_11\", task=task, selected_labels=selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe4d2",
   "metadata": {},
   "source": [
    "If you want to select labels to train on that do not belong to a specific category, you should select \"sc\" as the task. This will give you access to all labels. You can then combine the labels freely, for example \"Moralisierung\" and \"Fairness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"sc\"\n",
    "selected_labels = [\"Moralisierung\", \"Fairness\"] # select the specified labels you want to train on from the set of all labels\n",
    "data_manager = DataManager(\"/content/data/Test_Data/XMI_11\", task=task, selected_labels=selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40269a",
   "metadata": {},
   "source": [
    "## Prepare the data in dataset format\n",
    "The data is read in as xmi and then converted to a spacy doc object. This is done so we can specify the spans in the flowing text; and also that sentence boundaries are detected. For the transformers models, we feed the data in chunks, and currently each sentence is a chunk. One could also think about different choices such as paragraphs or instances.\n",
    "\n",
    "The doc object is generated by the `DataManager`. We then need to use the transformers specific methods in the `TransformersDataHandler` to create nested lists of tokens (nesting by sentences, these are the \"chunks\"), and make sure that the labels for the selected annotation are nested in the same way. The labels that are then assigned are \"2\" for the first token in an annotation, \"1\" for a token inside an annotation, \"0\" for no annotation, \"-100\" for punctuation marks as these should be ignored in the calculation of the loss function (cross entropy).\n",
    "This is all taken care of by the `DataManager`.\n",
    "\n",
    "1. xmi data -> spacy doc object\n",
    "2. get tokens, sentences and labels from spacy doc object and put in nested lists\n",
    "3. Nested lists into dataframe\n",
    "\n",
    "The pandas dataframe can then be exported into a Hugging Face dataset and can be pushed to the hub.\n",
    "\n",
    "4. Dataframe to dataset\n",
    "5. Optional: Publish dataset on hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset from the dataframe and split the data into test and training set\n",
    "data_manager.df_to_dataset(split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e90ee7",
   "metadata": {},
   "source": [
    "You can now publish the dataset to the Hugging Face Hub. For this you either need to set the environment variable `HUGGING_FACE_TOKEN` or you can provide it here using the `hugging_face_token` keyword. The `repo_id` variable specifices the name of the repository that you want to use (or create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now push to hub\n",
    "data_manager.publish(repo_id=\"test-data-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe29b75",
   "metadata": {},
   "source": [
    "You can also update the metadata in the `DatasetInfo` object that goes along with your dataset. Possible options to update are `description`, `version`, `license`, `citation`, `homepage`. You can update one or several of these, or all of them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset = data_manager.set_dataset_info(version=\"0.0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335de859",
   "metadata": {},
   "source": [
    "To update the dataset on Hugging Face Hub, you may now push this updated dataset, directly providing the updated dataset as a keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.publish(repo_id=\"test-data-3\", data_set=updated_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee2e70",
   "metadata": {},
   "source": [
    "## Pull an existing dataset from Hugging Face\n",
    "Instead of creating a dataset from your own annotated data, you may also load a dataset from Hugging Face. For this, when initializing the DataManager, you need to set `skip_read` so that the DataManager does not attempt to read data from the provided directory. Instead, the dataset that you pull from Hugging Face will be saved to the provided directory. Further, you need to specify the name of the dataset, the split you want to load (\"train\" or \"test\") and optionally a revision number if you do not want to load the current default version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42de696",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = DataManager(\"../data/Test_Data/\", skip_read=True)\n",
    "dataset = data_manager.pull_dataset(dataset_name=\"conllpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd19a2",
   "metadata": {},
   "source": [
    "You can inspect the loaded dataset by looking at its DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.data_in_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff101",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.data_in_frame.ner_tags.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8973c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.train_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a0319",
   "metadata": {},
   "source": [
    "## Get started with training a transformers model\n",
    "For this you need a model that you want to base your training on. You also need to provide the path to the directory where you want to save the model. The model name can be given using the `model_name` keyword. This keyword defaults to `bert-base-cased`. You should set the `label_names` as well if they differ from the three default names `0, M-BEG, M` (which stand for no moralization, beginning of moralization segment and continuing moralization segment).\n",
    "The language is determined by the model that you use. The default model is an English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm = TransformersModelManager(model_path=\".\", model_name=\"bert-base-cased\", label_names = [\"0\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"some\", \"other\", \"label\", \"here\"])\n",
    "# tmm = TransformersModelManager(model_path=\".\", model_name=\"bert-base-cased\", label_names = [\"0\", \"M\", \"M-BEG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4baecc",
   "metadata": {},
   "source": [
    "To train, simply call the `train` method with the above `data_manager`. The token and column names are passed using the `token_column_name` and  and `label_column_name` keywords. If the data has been prepared by the `DataManager` and was not a dataset you pulled from the Hugging Face Hub, these are set to `Sentences` and `Labels`. The number of training epochs is set by the keyword `num_train_epochs`.\n",
    "As optimizer we currently use AdamW. The learning rate can be adjusted directly using the `learning_rate` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79668aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_column_name = \"Sentences\"\n",
    "# label_column_name = \"Labels\"\n",
    "token_column_name = \"tokens\"\n",
    "label_column_name = \"ner_tags\"\n",
    "num_train_epochs = 1\n",
    "learning_rate = 1e-5\n",
    "tmm.train(data_manager, token_column_name, label_column_name, num_train_epochs, learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbc7ac",
   "metadata": {},
   "source": [
    "You can now evaluate the model with an example phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = tmm.evaluate(token=\"Jupyter Notebooks sind super.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe052e",
   "metadata": {},
   "source": [
    "Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in evaluation_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26404884",
   "metadata": {},
   "source": [
    "The model is now saved in your provided `model_path`. We will add a functionality to push the model to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0ed1c",
   "metadata": {},
   "source": [
    "### Edit metadata\n",
    "\n",
    "- `metadata` is a dictionary of metadata for the model\n",
    "- This is pre-set to initiate the tags on the Hugging Face hub\n",
    "- modify below to update the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmm.metadata.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe55987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.metadata.metadata[\"datasets\"] = \"conllpp\"\n",
    "tmm.metadata.metadata[\"language\"] = \"en\"\n",
    "tmm.metadata.metadata[\"license\"] = \"mit\"\n",
    "tmm.metadata.metadata[\"metrics\"] = \"seqeval\"\n",
    "tmm.metadata.metadata[\"tags\"] = [\"token-classification\"]\n",
    "tmm.metadata.metadata[\"thumbnail\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be1c8fa",
   "metadata": {},
   "source": [
    "Save the updated metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e74662",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33118cdc",
   "metadata": {},
   "source": [
    "### Publish to a new repository on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = tmm.publish(repo_name=\"test-other-dataset2\", hf_namespace=\"iulusoy\", create_new_repo=True)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1d8f2",
   "metadata": {},
   "source": [
    "### Publish to an existing repository on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b14d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = tmm.publish(repo_name=\"t2\", hf_namespace=\"iulusoy\", create_new_repo=False)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a85047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
