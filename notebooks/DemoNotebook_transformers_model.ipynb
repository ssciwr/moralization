{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cc9ef3",
   "metadata": {},
   "source": [
    "# Demo Notebook for transformers models\n",
    "*SSC, May 2023*\n",
    "\n",
    "This notebook demonstrates the preliminary use for training transformers models. For now, all the methods are called from the notebook. In the future, a more user-friendly user interface will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ignore this cell: extra install steps that are only executed when running the notebook on Google Colab\n",
    "# flake8-noqa-cell\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.isdir('Test_Data'):\n",
    "    # we're running on colab and we haven't already downloaded the test data\n",
    "    # first install pinned version of setuptools (latest version doesn't seem to work with this package on colab)\n",
    "    %pip install setuptools==61 -qqq\n",
    "    # install the moralization package\n",
    "    %pip install git+https://github.com/ssciwr/moralization.git -qqq\n",
    "\n",
    "    # download test data sets\n",
    "    !wget https://github.com/ssciwr/moralization/archive/refs/heads/test_data.zip -q\n",
    "    !mkdir -p data && unzip -qq test_data.zip && mv -f moralization-test_data/*_Data ./data/. && rm -rf moralization-test_data test_data.zip\n",
    "    !spacy download de_core_news_sm\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7beae1",
   "metadata": {},
   "source": [
    "Import the required classes from the moralization package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moralization import DataManager, TransformersDataHandler, TransformersModelManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d63a6",
   "metadata": {},
   "source": [
    "### Import training data using DataManager\n",
    "\n",
    "If you need more information about raised warnings run: <br>\n",
    "```import logging ``` <br>\n",
    "```logging.getLogger().setLevel(logging.DEBUG)```\n",
    "\n",
    "Note that currently only annotations of one file, the one specified in `example_name` (see below) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on small dataset\n",
    "# data_manager = DataManager(\"../../moralization_data/Test_Data/\")\n",
    "data_manager = DataManager(\"../data/Test_Data/XMI_11\")\n",
    "# train on full dataset\n",
    "# data_manager = DataManager(\"/content/data/All_Data/XMI_11\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, doc in data_manager.doc_dict.items():\n",
    "    print(f\"  - {title}: {len(doc)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40269a",
   "metadata": {},
   "source": [
    "## Prepare the data in dataset format\n",
    "The data is read in as xmi and then converted to a spacy doc object. This is done so we can specify the spans in the flowing text; and also that sentence boundaries are detected. For the transformers models, we feed the data in chunks, and currently each sentence is a chunk. One could also think about different choices such as paragraphs or instances.\n",
    "\n",
    "The doc object is generated by the `DataManager`. We then need to use the transformers specific methods in the `TransformersDataHandler` to create nested lists of tokens (nesting by sentences, these are the \"chunks\"), and make sure that the labels for the selected annotation are nested in the same way. The labels that are then assigned are \"2\" for the first token in an annotation, \"1\" for a token inside an annotation, \"0\" for no annotation, \"-100\" for punctuation marks as these should be ignored in the calculation of the loss function (cross entropy).\n",
    "\n",
    "1. xmi data -> spacy doc object\n",
    "2. get tokens, sentences and labels from spacy doc object and put in nested lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = \"test_data-trimmed_version_of-Interviews-pos-SH-neu-optimiert-AW\"\n",
    "# init the TransformersDataHandler\n",
    "tdh = TransformersDataHandler()\n",
    "# pass the dictionary of spacy doc objects to the TransformersDataHandler\n",
    "# select the file to be used by example_name\n",
    "tdh.get_data_lists(data_manager.doc_dict, example_name)\n",
    "tdh.generate_labels(data_manager.doc_dict, example_name)\n",
    "list_of_sentence_list_of_tokens, list_of_labels = tdh.structure_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e90ee7",
   "metadata": {},
   "source": [
    "We have now obtained our nested lists. We can check the first few items of them to see if they look ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd607bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(list_of_sentence_list_of_tokens[i])\n",
    "    print(list_of_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323303b0",
   "metadata": {},
   "source": [
    "Now we convert the nested lists into a pandas dataframe. This dataframe can then be exported into a Hugging Face dataset and can be pushed to the hub. This functionality is not implemented yet but will be in the future.\n",
    "\n",
    "3. Nested lists into dataframe\n",
    "4. Dataframe to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_frame = data_manager.lists_to_df(sentence_list=list_of_sentence_list_of_tokens, label_list=list_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can either obtain a raw dataset or one that is split into test and train\n",
    "# raw_dataset = data_manager.df_to_dataset(data_in_frame=data_in_frame, split=False)\n",
    "train_test_dataset = data_manager.df_to_dataset(data_in_frame=data_in_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a0319",
   "metadata": {},
   "source": [
    "## Initiate required elements for training\n",
    "Before the training, we have to tokenize the pre-tokenized data with the tokenizer that goes along with the selected model. You need to provide the path to the directory where you want to save the model. The model name can be given using the `model_name` keyword. This keyword defaults to `bert-base-cased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd455ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tmm = TransformersModelManager(model_path=\".\", model_name=model_name)\n",
    "tokenized_dataset = tmm.map_dataset(train_test_set=train_test_dataset, token_column_name=\"Sentences\", label_column_name=\"Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090eadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba2f7e",
   "metadata": {},
   "source": [
    "Now the data collator that forms the batches from the training and test data is initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ba34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.init_data_collator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d1a19",
   "metadata": {},
   "source": [
    "The metric is seleced and initiated. You may pass different label names using the `label_names` keyword argument. The default is `[\"0\", \"M\", \"M-BEG\"]` for the labels 0, 1, 2 that are used to designate no moralization, moralization, beginning of a moralization. The metric that is chosen is `seqeval` but can be changed using the eval_metric keyword. See https://huggingface.co/docs/evaluate/choosing_a_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.load_evaluation_metric()\n",
    "# we need to map ids and labels for the model\n",
    "tmm.set_id2label()\n",
    "tmm.set_label2id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c200f1",
   "metadata": {},
   "source": [
    "Now we load the model. This uses the model name from above. You can load a different model than the tokenizer is from, this is however not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd933c9b",
   "metadata": {},
   "source": [
    "Now we load the dataloader that handles the loading of the data into the batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.load_dataloader(tokenized_datasets=tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4baecc",
   "metadata": {},
   "source": [
    "Now load the optimizer; we use AdamW for this. Learning rate can be adjusted directly using the `learning_rate` keyword, all other arguments can be passed as a dictionary if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79668aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.load_optimizer(learning_rate=2e-5, kwargs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f81776",
   "metadata": {},
   "source": [
    "Now load the accelerator that makes use of the existing hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc90d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.load_accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927af5f2",
   "metadata": {},
   "source": [
    "Load the scheduler that handles the adjustment of the learning rate during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.load_scheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4f550",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d023c",
   "metadata": {},
   "source": [
    "Evaluate the model, providing the `model_path` of the trained model and a sample phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26163aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = tmm.evaluate(token=\"Jupyter Notebooks sind super.\", model_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in evaluation_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21ebc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
