{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export:\n",
    "\n",
    "\n",
    "So far the only export I could get working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassis import load_typesystem, load_cas_from_xmi\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Documentation for cassis\n",
    "# https://github.com/dkpro/dkpro-cassis\n",
    "\n",
    "\n",
    "# might be usefull:\n",
    "# https://spacy.io/api/textcategorizer\n",
    "# https://spacy.io/universe/project/classyclassification\n",
    "# https://stackoverflow.com/questions/62075223/how-to-improve-a-german-text-classification-model-in-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the filesystem and the actual annotated text file.\n",
    "# The file system includes all possible types that are configured in inception.\n",
    "# This should only need one file for the entire dataset.\n",
    "\n",
    "with open(\"../data/TypeSystem.xml\", \"rb\") as f:\n",
    "    ts = load_typesystem(f)\n",
    "\n",
    "with open(\"../data/Gerichtsurteile-pos-AW-neu-optimiert-BB.xmi\", \"rb\") as f:\n",
    "    cas = load_cas_from_xmi(f, typesystem=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all custom Spans and store them in an ordered dict,\n",
    "# where the first dimension is the used inception category (Protagonistinnen, Forderung, etc...)\n",
    "# and the second dimension is the corresponding value of this category ('Forderer:in', 'Adresassat:in', 'Benefizient:in')\n",
    "# dict[category][entry value] = span\n",
    "def sort_spans(cas, ts):\n",
    "\n",
    "    span_type = ts.get_type(\"custom.Span\")\n",
    "    span_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # list of all interesting categories\n",
    "    cat_list = [\n",
    "        \"Protagonistinnen\",\n",
    "        \"Protagonistinnen2\",\n",
    "        \"Protagonistinnen3\",\n",
    "        \"Forderung\",\n",
    "        \"KAT1MoralisierendesSegment\",\n",
    "        \"KAT2Subjektive_Ausdrcke\",\n",
    "        \"KOMMENTAR\",\n",
    "        \"KommunikativeFunktion\",\n",
    "        \"Moralwerte\",\n",
    "    ]\n",
    "\n",
    "    for span in cas.select(span_type.name):\n",
    "\n",
    "        for cat in cat_list:\n",
    "            # this excludes any unwanted datapoints\n",
    "            if span[cat] and span[\"KOMMENETAR\"] != \"Dopplung\":\n",
    "\n",
    "                span_dict[cat][span[cat]].append(span)\n",
    "\n",
    "    # for span_dict_key, span_dict_sub_kat in span_dict.items():\n",
    "    #     print(f\"{span_dict_key}: {[key for key in span_dict_sub_kat.keys()]}\")\n",
    "    return span_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load multiple files into a list of dictionaries\n",
    "def load_data_dir(dir_path):\n",
    "    data_files = glob.glob(os.path.join(dir_path, \"*.xmi\"))\n",
    "\n",
    "    with open(glob.glob(os.path.join(dir_path, \"*.xml\"))[0], \"rb\") as f:\n",
    "        ts = load_typesystem(f)\n",
    "    data_dict_list = {}\n",
    "    for data_file in data_files:\n",
    "        # the wikipediadiskussionen file breaks as it has an invalid xmi charakter.\n",
    "        if data_file != \"../data/Wikipediadiskussionen-neg-BD-neu-optimiert-CK.xmi\":\n",
    "            with open(data_file, \"rb\") as f:\n",
    "                cas = load_cas_from_xmi(f, typesystem=ts)\n",
    "            data_dict_list[os.path.basename(data_file).split(\".xmi\")[0]] = {\n",
    "                \"data\": sort_spans(cas, ts),\n",
    "                \"file_type\": os.path.basename(data_file).split(\".\")[1],\n",
    "            }\n",
    "\n",
    "    return data_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the overlaying category for an second dimension cat name\n",
    "def find_cat_from_str(cat_entry, span_dict):\n",
    "\n",
    "    for span_dict_key, span_dict_sub_kat in span_dict.items():\n",
    "        if cat_entry in span_dict_sub_kat.keys():\n",
    "            return span_dict_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get overlap%\n",
    "# so far this only works on a span basis and not a sentance basis.\n",
    "\n",
    "\n",
    "def get_overlap_percent(cat_1, cat_2, data_dict_list, file_name, ret_occ=False):\n",
    "    o_cat1 = find_cat_from_str(cat_1, data_dict_list[file_name][\"data\"])\n",
    "    o_cat2 = find_cat_from_str(cat_2, data_dict_list[file_name][\"data\"])\n",
    "\n",
    "    occurence = 0\n",
    "    total = 0\n",
    "    for span in data_dict_list[file_name][\"data\"][o_cat1][cat_1]:\n",
    "        total += 1\n",
    "        if span[o_cat2] == cat_2:\n",
    "            occurence += 1\n",
    "    if ret_occ:\n",
    "        return occurence, total\n",
    "    else:\n",
    "        return round(occurence / total, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_matrix(data_dict_list, file_name, cat_list=None):\n",
    "    if cat_list is None:\n",
    "        cat_list = []\n",
    "        for span_dict_key, span_dict_sub_kat in data_dict_list[file_name][\n",
    "            \"data\"\n",
    "        ].items():\n",
    "            [cat_list.append(key) for key in span_dict_sub_kat.keys()]\n",
    "\n",
    "    percent_matrix = np.zeros((len(cat_list), len(cat_list)))\n",
    "    for i, cat1 in enumerate(cat_list):\n",
    "        for j, cat2 in enumerate(cat_list):\n",
    "            percent_matrix[i, j] = get_overlap_percent(\n",
    "                cat1, cat2, data_dict_list, file_name\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(percent_matrix, index=cat_list)\n",
    "    df.columns = cat_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode can be \"instances\" or \"span\"\n",
    "# instances reports the number of occurences and span\n",
    "\n",
    "\n",
    "def report_instances(data_dict_list, file_names=None):\n",
    "\n",
    "    if file_names is None:\n",
    "        file_names = list(data_dict_list.keys())\n",
    "    elif isinstance(file_names, str):\n",
    "        file_names = [file_names]\n",
    "\n",
    "    # filename: main_cat: sub_cat: instances\n",
    "    instance_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for file_name in file_names:\n",
    "        span_dict = data_dict_list[file_name][\"data\"]\n",
    "        # initilize total instances rows for easier setting later.\n",
    "        instance_dict[file_name][(\"total instances\", \"with invalid\")] = 0\n",
    "        instance_dict[file_name][(\"total instances\", \"without invalid\")] = 0\n",
    "\n",
    "        for main_cat_key, main_cat_value in span_dict.items():\n",
    "            for sub_cat_key, sub_cat_value in main_cat_value.items():\n",
    "                # the tuple index makes it easy to convert the dict into a pandas dataframe\n",
    "                instance_dict[file_name][(main_cat_key, sub_cat_key)] = len(\n",
    "                    sub_cat_value\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(instance_dict)\n",
    "    df.index = df.index.set_names(([\"Main Category\", \"Sub Category\"]))\n",
    "\n",
    "    # add rows for total instances\n",
    "    df.loc[(\"total instances\", \"with invalid\"), :] = df.sum(axis=0).values\n",
    "    df.loc[(\"total instances\", \"without invalid\"), :] = (\n",
    "        df.loc[(\"total instances\", \"with invalid\"), :].values\n",
    "        - df.loc[\"KAT1MoralisierendesSegment\", \"Keine Moralisierung\"].values\n",
    "    )\n",
    "\n",
    "    # sort by index and occurence number\n",
    "    df = df.sort_values(\n",
    "        by=[\n",
    "            \"Main Category\",\n",
    "            \"Sub Category\",\n",
    "            file_names[0],\n",
    "        ],\n",
    "        ascending=False,\n",
    "    )\n",
    "\n",
    "    # fill NaN\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_spans(data_dict_list, file_names=None):\n",
    "\n",
    "    if file_names is None:\n",
    "        file_names = list(data_dict_list.keys())\n",
    "    elif isinstance(file_names, str):\n",
    "        file_names = [file_names]\n",
    "\n",
    "    df_spans = report_instances(data_dict_list, file_names)\n",
    "    # this report_instances call makes it much easier to include the total number of spans for each columns, as well as removes the need to duplicate the pandas setup.\n",
    "\n",
    "    df_spans[:] = df_spans[:].astype(\"object\")\n",
    "    for file_name in file_names:\n",
    "        span_dict = data_dict_list[file_name][\"data\"]\n",
    "\n",
    "        for main_cat_key, main_cat_value in span_dict.items():\n",
    "            for sub_cat_key, sub_cat_value in main_cat_value.items():\n",
    "\n",
    "                # multiple options for how to report the spans are available\n",
    "\n",
    "                # first report the entire span object as a string\n",
    "                span_list = [str(span) for span in span_dict[main_cat_key][sub_cat_key]]\n",
    "                # this would look like this:\n",
    "                # c.Span(Protagonistinnen=Forderer:in, Protagonistinnen2=Individuum, Protagonistinnen3=Own Group, begin=21822, end=21874);\n",
    "                # c.Span(Protagonistinnen=Benefizient:in, Protagonistinnen2=Institution, Protagonistinnen3=Own Group, begin=21974, end=21984);\n",
    "                # c.Span(Protagonistinnen=Forderer:in, Protagonistinnen2=Institution, Protagonistinnen3=Own Group, begin=66349, end=66352)\n",
    "                # maybe one should remove the c.Span() but i'm not sure what exactly is wanted here.\n",
    "\n",
    "                # second option is to report the end or beginning index for each span\n",
    "                # span_list=[str(span[\"end\"]) for span in span_dict[main_cat_key][sub_cat_key] ]\n",
    "\n",
    "                # convert list to seperated str\n",
    "\n",
    "                span_str = \";\".join(span_list)\n",
    "                span_str = span_str.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "                df_spans.at[\n",
    "                    (main_cat_key, sub_cat_key),\n",
    "                    file_name,\n",
    "                ] = span_str\n",
    "\n",
    "    return df_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = load_data_dir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i would guess from here one can get the correlation table somehow.\n",
    "# just df.corr() yields unsatisfactory results..\n",
    "\n",
    "# these DataFrames can now easily be saved as a csv file.\n",
    "df_instances = report_instances(data_dict_list)\n",
    "df_instances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this df can now easily be filtered.\n",
    "df_instances.loc[\"KAT2Subjektive_Ausdrcke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spans = report_spans(data_dict_list)\n",
    "df_spans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_overlap_percent(\n",
    "    \"Forderer:in\", \"Neutral\", data_dict_list, \"Gerichtsurteile-neg-AW-neu-optimiert-BB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_percent_matrix(data_dict_list, \"Gerichtsurteile-neg-AW-neu-optimiert-BB\")\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.heatmap(df, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = get_percent_matrix(\n",
    "    data_dict_list,\n",
    "    \"Gerichtsurteile-neg-AW-neu-optimiert-BB\",\n",
    "    [\n",
    "        \"Appell\",\n",
    "        \"Adresassat:in\",\n",
    "        \"Forderer:in\",\n",
    "        \"soziale Gruppe\",\n",
    "        \"Benefizient:in\",\n",
    "        \"Neutral\",\n",
    "        \"Institution\",\n",
    "        \"Expression\",\n",
    "    ],\n",
    ")\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = sns.heatmap(df_small, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('moralization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eecdbd9102a8a153b703175e0caf5270cf41b05f4ed4e7eba6bc686781fa17e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
