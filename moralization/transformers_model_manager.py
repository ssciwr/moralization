from pathlib import Path
from typing import Union, Optional


class TransformersModelManager:
    """
    Create, import, modify, train and publish transformer models.

    Models can be trained on data from a DataManager, and published to Hugging Face.
    """

    def __init__(
        self,
        data_path: Union[str, Path],
        overwrite_existing_files: bool = False,
    ):
        """
        Imports an existing model from the `model_path` folder if found.

        If the `model_path` folder does not exist, or if `base_config_file`
        is supplied, or if `overwrite_existing_files` is True,
        creates a new model in the `model_path` folder.

        Resulting folder structure inside `model_path`:

        - /config.cfg: model config file
        - /meta.json: user-editable metadata (also exported to trained models)

        If the model has been trained the folder will also contain:

        - /data/train.spacy: training dataset generated by DataManager
        - /data/dev.spacy: testing dataset generated by DataManager
        - /model-best/config.cfg: best trained model
        - /model-last/config.cfg: last trained model

        Args:
            model_path (str or Path): Folder where the model is (or will be) stored
            base_config_file (str or Path, optional): If supplied this base config will be used to create a new model
            overwrite_existing_files (bool): If true any existing files in `model_path` are removed
        """
        self._model_path = Path(model_path)
        self._best_model_path = self._model_path / "model-best"
        self._last_model_path = self._model_path / "model-last"
        existing_model = (
            self._model_path.is_dir() and (self._model_path / "config.cfg").is_file()
        )
        if base_config_file or overwrite_existing_files or not existing_model:
            _create_model(self._model_path, base_config_file, overwrite_existing_files)
        self.metadata = _import_or_create_metadata(self._model_path)

    def init_model(self, model_name="bert-base-cased"):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        except ValueError:
            raise ValueError(
                "Could not initiate tokenizer - please check your model name"
            )
        if not self.tokenizer.is_fast:
            raise ValueError(
                "Please use a different model that provices a fast tokenizer"
            )

    def __repr__(self) -> str:
        name = f"{self.metadata['name']}-{self.metadata['version']}"
        path = f"{self._model_path.resolve()}"
        return f"SpacyModelManager('{path}' [{name}])"

    def train(
        self,
        data_manager: DataManager,
        use_gpu: int = -1,
        overrides: Optional[Dict] = None,
    ):
        """Train the model on the data contained in `data_manager`.

        Args:
            data_manager (DataManager): the DataManager that contains the training data
            use_gpu (int): The index of the GPU to use (default: -1 which means no GPU)
            overrides (dict): An optional dictionary of parameters to override in the model config
        """
        self.save()
        if overrides is None:
            overrides = {}
        # use data from data_manager for training
        data_train, data_dev = self._get_data_manager_docbin_files(data_manager)
        overrides["paths.train"] = str(data_train)
        overrides["paths.dev"] = str(data_dev)
        spacy.cli.train.train(
            self._model_path / "config.cfg",
            self._model_path,
            use_gpu=use_gpu,
            overrides=overrides,
        )

    def evaluate(self, data_manager: DataManager) -> Dict[str, Any]:
        """Evaluate the model against the test dataset in `data_manager`"""
        self._check_model_is_trained_before_it_can_be("evaluated")
        _, data_dev = self._get_data_manager_docbin_files(data_manager)
        return spacy.cli.evaluate(str(self._best_model_path), data_dev)

    def save(self):
        """Save any changes made to the model metadata."""
        self.metadata["name"] = _make_valid_package_name(self.metadata.get("name"))
        with open(self._model_path / "meta.json", "w") as f:
            json.dump(self.metadata, f)
        for model_path in [self._best_model_path, self._last_model_path]:
            _update_spacy_model_meta(model_path, self.metadata)

    def test(self, test_string: str, style: str = "span"):
        """Test the model output with a test string"""
        self._check_model_is_trained_before_it_can_be("tested")
        nlp = spacy.load(self._best_model_path)
        doc_dict = {"test_doc": nlp(test_string)}
        return visualize_data(doc_dict, style=style)

    def publish(self, hugging_face_token: Optional[str] = None) -> Dict[str, str]:
        """Publish the model to Hugging Face.

        This requires a User Access Token from https://huggingface.co/

        The token can either be passed via the `hugging_face_token` argument,
        or it can be set via the `HUGGING_FACE_TOKEN` environment variable.

        Args:
            hugging_face_token (str, optional): Hugging Face User Access Token
        Returns:
            dict: URLs of the published model and the pip-installable wheel
        """
        self._check_model_is_trained_before_it_can_be("published")
        for key, value in self.metadata.items():
            if value == "":
                raise RuntimeError(
                    f"Metadata '{key}' is not set - all metadata needs to be set before publishing a model."
                )
        self.save()
        if hugging_face_token is None:
            hugging_face_token = os.environ.get("HUGGING_FACE_TOKEN")
        if hugging_face_token is None:
            raise ValueError(
                "API TOKEN required: pass as string or set the HUGGING_FACE_TOKEN environment variable."
            )
        huggingface_hub.login(token=hugging_face_token)
        with tempfile.TemporaryDirectory() as tmpdir:
            # convert model to a python package including binary wheel
            package_path = Path(tmpdir)
            spacy.cli.package(self._best_model_path, package_path, create_wheel=True)
            # construct path to binary wheel
            nlp = spacy.load(self._best_model_path)
            wheel_path = _construct_wheel_path(package_path, nlp.meta)
            # push the package to hugging face
            return spacy_huggingface_hub.push(wheel_path)

    def _check_model_is_trained_before_it_can_be(self, action: str = "used"):
        if not self._best_model_path.is_dir():
            raise RuntimeError(f"Model must be trained before it can be {action}.")

    def _get_data_manager_docbin_files(self, data_manager: DataManager) -> List[Path]:
        """
        Returns `[train_data_path, dev_data_path]` from data_manager.

        If the supplied DataManager has no docbin files we first export them to `model_path/data`.
        """
        data_files = data_manager.spacy_docbin_files
        data_files_exist = data_files is not None and all(
            [data_file.is_file() for data_file in data_files]
        )
        if not data_files_exist:
            data_path = self._model_path.resolve() / "data"
            Path(data_path).mkdir(parents=True, exist_ok=True)
            data_manager.export_data_DocBin(data_path, overwrite=True)
        return data_manager.spacy_docbin_files
